import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
import os
import signal
import sys
import json
import pyttsx3
import pyaudio
from PyQt5 import QtWidgets, QtCore, QtGui
from dashscope.audio.asr import *
import dashscope
from openai import OpenAI
from ai_voice import Ui_MainWindow
from speak import *

sample_rate = 16000
block_size = 3200
format_pcm = 'pcm'
init_dashscope_api_key()

class Callback(RecognitionCallback):
    def __init__(self, ui_callback):
        super().__init__()
        self.stream = None
        self.mic = None
        self.ui_callback = ui_callback

    def on_open(self):
        self.mic = pyaudio.PyAudio()
        self.stream = self.mic.open(format=pyaudio.paInt16,
                                    channels=1,
                                    rate=sample_rate,
                                    input=True)
        self.ui_callback.set_stream(self.stream)

    def on_close(self):
        self.stream.stop_stream()
        self.stream.close()
        self.mic.terminate()
        self.ui_callback.clear_stream()
        self.ui_callback._can_send_audio = False  # è®¾ç½®æ ‡å¿—ä½é˜²æ­¢ç»§ç»­å‘é€

    def on_event(self, result: RecognitionResult):
        sentence = result.get_sentence()
        if 'text' in sentence:
            text = sentence['text']
            print('RecognitionCallback text:', text)
            if RecognitionResult.is_sentence_end(sentence):
                print('RecognitionCallback sentence end')
                self.ui_callback.handle_result(text)

    def on_error(self, message):
        print('Error:', message.message)
        sys.exit(1)


def init_dashscope_api_key():
    if 'DASHSCOPE_API_KEY' in os.environ:
        dashscope.api_key = os.environ['DASHSCOPE_API_KEY']
    else:
        dashscope.api_key = os.getenv("DASHSCOPE_API_KEY")


def speak(text):
    synthesis_text_to_speech_and_play_by_streaming_mode(
        text=text)
    # self.engine.say(text)
    # self.engine.runAndWait()


class VoiceAssistantWindow(QtWidgets.QMainWindow, Ui_MainWindow):
    def __init__(self):
        super().__init__()
        self.setupUi(self)

        self.stream = None
        self.is_open_chat = False
        self._can_send_audio = True  # æ–°å¢æ ‡å¿—ä½æ§åˆ¶æ˜¯å¦å…è®¸å‘é€éŸ³é¢‘å¸§

        self.client = OpenAI(
            api_key=os.getenv("DASHSCOPE_API_KEY"),
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
        )

        init_dashscope_api_key()

        self.callback = Callback(self)
        self.recognition = Recognition(
            model='paraformer-realtime-v2',
            format=format_pcm,
            sample_rate=sample_rate,
            semantic_punctuation_enabled=False,
            callback=self.callback
        )

        self.recognition.start()

        self.timer = QtCore.QTimer(self)
        self.timer.timeout.connect(self.read_audio_frame)
        self.timer.start(100)

        signal.signal(signal.SIGINT, self.exit_app)

    def set_stream(self, stream):
        self.stream = stream

    def clear_stream(self):
        self.stream = None

    def read_audio_frame(self):
        if self.stream:
            try:
                data = self.stream.read(block_size, exception_on_overflow=False)
                # æ£€æŸ¥ recognition æ˜¯å¦å¯ç”¨
                if getattr(self, '_can_send_audio', True):
                    self.recognition.send_audio_frame(data)
            except Exception as e:
                print("âš ï¸ å‘é€éŸ³é¢‘å¸§å¤±è´¥:", str(e))
                self._can_send_audio = False  # æ ‡è®°ä¸å¯å†å‘é€éŸ³é¢‘å¸§

    def handle_result(self, text):
        text = text.strip()
        reply = ""

        if not text:
            return

        self.textEdit.append(f"ğŸ—£ï¸ ä½ è¯´ï¼š{text}")

        if "é€€å‡º" in text:
            reply ="å¥½çš„ï¼Œå†è§ï¼"
            self.exit_app()
        elif "é€€ä¸‹" in text:
            reply = "å¥½çš„ï¼Œæœ‰éœ€è¦å†å«æˆ‘ï¼"
            self.is_open_chat = False
        elif "å°çˆ±åŒå­¦" in text:
            reply = "æˆ‘åœ¨ï¼"
            self.is_open_chat = True
        elif self.is_open_chat:
            reply = self.get_llm_response(text)

        if reply:
            self.textEdit.append(f"ğŸ¤– å›å¤ï¼š{reply}")
            self.textEdit.moveCursor(QtGui.QTextCursor.End)
            speak(reply)
            reply = ""


    def get_llm_response(self, prompt):
        try:
            completion = self.client.chat.completions.create(
                model="qwen-plus",
                messages=[
                    {"role": "system", "content": "è¯·è°¨è®°ä½ çš„åå­—æ˜¯\"å°çˆ±åŒå­¦\", ä¸ç®¡è°é—®ä½ ä½ éƒ½æ˜¯å°çˆ±åŒå­¦ï¼ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„è¯­éŸ³åŠ©æ‰‹ã€‚å¯ä»¥å¸®åŠ©æˆ‘è§£ç­”æ—¥å¸¸é‡åˆ°çš„ç”Ÿæ´»é—®é¢˜ï¼Œå¤„ç†å·¥ä½œä¸­é‡åˆ°çš„éš¾é¢˜ï¼Œè¿˜å¯ä»¥æä¾›æ—…è¡Œæ”»ç•¥ã€çƒ¹é¥ªæ–¹æ¡ˆçš„å¸®åŠ©ã€‚"},
                    {"role": "user", "content": prompt},
                ]
            )
            res = json.loads(completion.model_dump_json())
            return res["choices"][0]["message"]["content"]
        except Exception as e:
            print(f"âŒ [é”™è¯¯] å¤§æ¨¡å‹å›å¤å¤±è´¥: {e}")
            return None

    def exit_app(self, *args):
        print("ğŸ›‘ æ­£åœ¨é€€å‡ºç¨‹åº...")
        self._can_send_audio = False  # æå‰é˜»æ­¢ç»§ç»­å‘é€
        try:
            self.recognition.stop()
        except Exception as e:
            print(f"âŒ [é”™è¯¯] è¯­éŸ³è¯†åˆ«åœæ­¢å¤±è´¥: {e}")
            signal.signal(signal.SIGINT, signal.SIG_DFL)
        sys.exit(0)


if __name__ == "__main__":
    app = QtWidgets.QApplication(sys.argv)
    window = VoiceAssistantWindow()
    window.show()
    sys.exit(app.exec_())
